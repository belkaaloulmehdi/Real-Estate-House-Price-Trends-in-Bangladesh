{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/belkaaloulmehdi/projects/blob/main/Copie_de_Demo_Day_Notebook_dse_ft_95_Groupe_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_tGRPTGP8F"
      },
      "source": [
        "# Real Estate & House Price Trends in Bangladesh::\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPHXpFGMy-2p"
      },
      "source": [
        "# **Data Exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCgr3xuHmr_y"
      },
      "source": [
        "## Libraries import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eeEs_gesGP8H"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as stats\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import libraries for Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhB3kkQZOx7v"
      },
      "source": [
        "## Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "DXZdqWmlGP8I",
        "outputId": "75330cb2-f47f-4046-904d-a9e14d920969"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/house_price_bd.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e825e7a463e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import & visualize dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/house_price_bd.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/house_price_bd.csv'"
          ]
        }
      ],
      "source": [
        "# Import & visualize dataset\n",
        "df = pd.read_csv(\"/content/house_price_bd.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-41NBDB1JH-x"
      },
      "outputs": [],
      "source": [
        "# Print the shape of dataset in the form of (#rows, #columns)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSfEVbgBeFiT"
      },
      "outputs": [],
      "source": [
        "# We use the function describe to have the main statistics information\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhPG942nm0EE"
      },
      "source": [
        "**==> We noticed that 75% of the properties have >= 3 bedrooms >= 3 bathrooms and floor_area < 2000 sqft**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHnA8_mbT0vf"
      },
      "outputs": [],
      "source": [
        "df_count = df.groupby(\"Bedrooms\").size().reset_index(name=\"count\")\n",
        "sns.barplot(x=\"Bedrooms\", y=\"count\", data=df_count)\n",
        "plt.xlim(-1, 7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAWCrkZeT3-7"
      },
      "outputs": [],
      "source": [
        "df_count = df.groupby(\"Bathrooms\").size().reset_index(name=\"count\")\n",
        "sns.barplot(x=\"Bathrooms\", y=\"count\", data=df_count)\n",
        "plt.xlim(-1, 7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rEZ_gwJT5lW"
      },
      "outputs": [],
      "source": [
        "bins = np.arange(0, 3000, 100)\n",
        "\n",
        "\n",
        "df_count = df.groupby(\"Floor_area\").size().reset_index(name=\"count\")\n",
        "df_count[df_count[\"Floor_area\"] <= 3000]\n",
        "df_count = pd.cut(df_count[\"Floor_area\"], bins).value_counts().reset_index(name=\"count\")\n",
        "plt.figure(figsize=(10,6))  # Set the figure size to 10 inches wide and 6 inches tall\n",
        "sns.barplot(x=\"Floor_area\", y=\"count\", data=df_count, color = \"red\", )\n",
        "plt.xticks(np.arange(0, len(df_count), step=2))  # Affiche une Ã©tiquette sur deux\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels by 45 degrees\n",
        "plt.tight_layout()  # Adjust the layout to fit the labels\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pApTwhJKGP8J"
      },
      "outputs": [],
      "source": [
        "# We use this function to have the number and Dtype of the columns\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2RnDfulnc6Y"
      },
      "source": [
        "**==> We noticed that Floor_no and Price_in_taka are in object type**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF00Vfb_K9WE"
      },
      "outputs": [],
      "source": [
        "# We look at the values of each column\n",
        "variables_list = df[[\"Bedrooms\",\"Bathrooms\", \"Floor_no\", \"Occupancy_status\", \"Floor_area\", \"City\", \"Price_in_taka\"]]\n",
        "for value in variables_list:\n",
        "    print(f'Possible value an doccurence for {value} variable are :')\n",
        "    print(df[[value]].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBHvZLKioCFE"
      },
      "source": [
        "**==> We need to clean Floor_no and Price_in_taka.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNoTNspLFGSZ"
      },
      "outputs": [],
      "source": [
        "# We checked missing data\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szTD1ZK2Ru_k"
      },
      "outputs": [],
      "source": [
        "# We checked duplicates\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52G8CzZ_PUOn"
      },
      "outputs": [],
      "source": [
        "# Remove duplicates\n",
        "df = df.drop_duplicates(keep='last')\n",
        "\n",
        "# \"Title\" and \"Location\" column drop\n",
        "df = df.drop([\"Title\", \"Location\"], axis=1)\n",
        "\n",
        "# Delete rows in [8th,4th to 8th Backside,A1,A2,A3,A4,A5,A6,A7,0+7,5th,1st,1F,G+7] corresponding to full building\n",
        "floor_to_remove = [\"Merin City - Purbach\", \"4th to 8th Backside\", \"G+7\", \"0+7\",\"A1,A2,A3,A4,A5,A6,A7\"]\n",
        "df = df[~df[\"Floor_no\"].isin(floor_to_remove)]\n",
        "\n",
        "# Replace 8th, 1st, 1F, 5th\n",
        "df.loc[df[\"Floor_no\"] == \"8th\", \"Floor_no\"] = 8\n",
        "df.loc[df[\"Floor_no\"] == \"1st\", \"Floor_no\"] = 1\n",
        "df.loc[df[\"Floor_no\"] == \"1F\", \"Floor_no\"] = 1\n",
        "df.loc[df[\"Floor_no\"] == \"5th\", \"Floor_no\"] = 5\n",
        "\n",
        "# Remove rows where both Bedrooms and Bathrooms are null and switch to int because we consider it as residential plots or/and commercial units\n",
        "df = df[(df[\"Bedrooms\"].notnull()) | (df[\"Bathrooms\"].notnull())]\n",
        "df = df[(df[\"Floor_no\"].notnull())]\n",
        "df[\"Bedrooms\"] = df[\"Bedrooms\"].astype(int)\n",
        "df[\"Bathrooms\"] = df[\"Bathrooms\"].astype(int)\n",
        "df[\"Floor_area\"] = df[\"Floor_area\"].astype(int)\n",
        "df[\"Floor_no\"] = df[\"Floor_no\"].astype(str)\n",
        "\n",
        "# Remove rows where 200 > Floor_area > 3000, Bedrooms > 6, and Bathrooms > 6 as we consider them as Outliers\n",
        "df = df[(df[\"Floor_area\"] > 200) & (df[\"Floor_area\"] <= 3000)]\n",
        "df = df[df['Bedrooms'] < 4]\n",
        "df = df[df['Bathrooms'] < 5]\n",
        "\n",
        "# Price column to remove à§³ and convert \"Price_in_taka\" column into float64\n",
        "df[\"Price_in_taka\"] = df[\"Price_in_taka\"].apply(lambda x: x.replace(\"à§³\", \"\").replace(\",\", \"\"))\n",
        "df[\"Price_in_taka\"] = df[\"Price_in_taka\"].astype(np.float64)\n",
        "\n",
        "# Check cleaned dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw5NkZ_CR3iD"
      },
      "outputs": [],
      "source": [
        "# Check duplicates\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkeqW_2qndCH"
      },
      "outputs": [],
      "source": [
        "# Data clean check\n",
        "variables_list = df[[\"Bedrooms\",\"Bathrooms\", \"Floor_no\", \"Floor_area\", \"City\", \"Occupancy_status\"]]\n",
        "for value in variables_list:\n",
        "    print(f'Possible value an doccurence for {value} variable are :')\n",
        "    print(df[[value]].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaakQI4louon"
      },
      "outputs": [],
      "source": [
        "# Check missing data\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azYnLZ_PQB4N"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP8GBgGSQZZF"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6bzNcOS-37p"
      },
      "outputs": [],
      "source": [
        "'''df_sorted = df.sort_values(by='Price_in_taka', ascending=False)\n",
        "sns.catplot(x=\"City\", y=\"Price_in_taka\",data=df_sorted, kind=\"bar\")'''\n",
        "\n",
        "df_sorted = df.sort_values(by='Price_in_taka', ascending=False)\n",
        "g = sns.catplot(x=\"City\", y=\"Price_in_taka\",data=df_sorted, kind=\"bar\")\n",
        "g.set_xticklabels(rotation=45, ha='right')  # Rotation des Ã©tiquettes\n",
        "plt.tight_layout()  # Ajustement automatique des Ã©lÃ©ments\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8zUrsh2_BST"
      },
      "outputs": [],
      "source": [
        "# Scatterplot with a Trend Line\n",
        "sns.regplot(x=\"Floor_area\", y=\"Price_in_taka\", scatter=True, color=\"#42A5F5\", line_kws={'color':'blue'}, data=df)\n",
        "\n",
        "# Add Labels and Title\n",
        "plt.xlabel('Floor Area')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Scatterplot avec une courbe de tendance')\n",
        "\n",
        "# Display graph\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhDay5g5yME7"
      },
      "source": [
        "# **Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oeBTOxGP9VF"
      },
      "source": [
        "## Preprocessing ðŸ³"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F7TusRRGP8J"
      },
      "outputs": [],
      "source": [
        "# Separate target variable Y from features X\n",
        "print(\"Separating target variable from features...\")\n",
        "\n",
        "# Choose the columns we want to have as our features\n",
        "features_list = [\"Bedrooms\",\"Bathrooms\",\"Floor_no\",\"Floor_area\",\"City\", \"Occupancy_status\"]\n",
        "\n",
        "# We add feature_list into our loc\n",
        "X = df.loc[:,features_list]\n",
        "\n",
        "# We set \"Price_in_taka\" as the target variable\n",
        "y = df.loc[:,\"Price_in_taka\"]\n",
        "\n",
        "print(\"...Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s95_sG9xGP8J"
      },
      "outputs": [],
      "source": [
        "# Divide dataset Train set & Test set\n",
        "print(\"Splitting dataset into train set and test set...\")\n",
        "\n",
        "# Then we use train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=0)\n",
        "\n",
        "print(\"...Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nIrg8HKarBE"
      },
      "outputs": [],
      "source": [
        "### Training pipeline ###\n",
        "print(\"--- Training pipeline ---\")\n",
        "print()\n",
        "\n",
        "# Encoding categorical features and standardizing numeric features\n",
        "print(\"#### X_train BEFORE preprocessing ####\")\n",
        "print(X_train.head())\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu6AVDBZLYkc"
      },
      "outputs": [],
      "source": [
        "categorical_column = X.select_dtypes(object).columns\n",
        "print(categorical_column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DGvngTYLq47"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qOqiLX7SaXm"
      },
      "outputs": [],
      "source": [
        "print(\"Encoding categorical features and standardizing numerical features...\")\n",
        "\n",
        "## StandardScaler to scale data (i.e apply Z-score)\n",
        "## OneHotEncoder to encode categorical variables\n",
        "\n",
        "numerical_column = X.select_dtypes(np.number).columns\n",
        "categorical_column = X.select_dtypes(object).columns\n",
        "\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore') #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yROI4cFSaRe"
      },
      "outputs": [],
      "source": [
        "# Apply ColumnTransformer to create a pipeline that will apply the above preprocessing\n",
        "feature_encoder = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', categorical_transformer, categorical_column),\n",
        "        ('num', numeric_transformer, numerical_column)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "X_train = feature_encoder.fit_transform(X_train)\n",
        "print(\"...Done.\")\n",
        "print(\"#### X_train AFTER preprocessing ####\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXRWcMO8GP8J"
      },
      "outputs": [],
      "source": [
        "### Testing pipeline ###\n",
        "print(\"--- Testing pipeline ---\")\n",
        "\n",
        "# Standardizing numeric features\n",
        "print(\"Standardizing numerical features...\")\n",
        "\n",
        "X_test = feature_encoder.transform(X_test)\n",
        "\n",
        "print(\"...Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ZQtfmFGP8K"
      },
      "source": [
        "## Build the model ðŸ‹ï¸â€â™‚ï¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO7teIuXGP8K"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "print(\"Train model...\")\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train) # This steps is the actual training !\n",
        "print(\"...Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqYezMIZGP8K"
      },
      "outputs": [],
      "source": [
        "# Predictions on training set\n",
        "print(\"Predictions on training set...\")\n",
        "y_train_pred = regressor.predict(X_train)\n",
        "print(\"...Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2RlqIHPHNsx"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXvryQJlGP8L"
      },
      "outputs": [],
      "source": [
        "# Predictions on test set\n",
        "print(\"Predictions on test set...\")\n",
        "y_test_pred = regressor.predict(X_test)\n",
        "print(\"...Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1V3JKNSGP8L"
      },
      "source": [
        "## Evaluate the model ðŸŒ¡ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_3b2AygGP8L"
      },
      "outputs": [],
      "source": [
        "# Performance assessment\n",
        "print(\"--- Assessing the performances of the model ---\")\n",
        "\n",
        "# Print R^2 scores\n",
        "print(\"R2 score on training set : \", regressor.score(X_train, y_train))\n",
        "print(\"R2 score on test set : \", regressor.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXLsn7DfGP8M"
      },
      "source": [
        "Depending on the results, we will be able to tell if the model is performing well and whether it is overfitting or not.\n",
        "\n",
        "* $R^2$ close to 1 means good performance\n",
        "* $R^2_{train}$ > $R^2_{test}$ means overfitting\n",
        "* $R^2_{train}$ < $R^2_{test}$ means underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxosWuSDGP8M"
      },
      "source": [
        "## Feature importance ðŸ¥•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL1c4bX1GP8M"
      },
      "outputs": [],
      "source": [
        "print(\"coefficients are: \", regressor.coef_)\n",
        "print(\"Constant is: \", regressor.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXHbkIfYGP8M"
      },
      "source": [
        "Now that we have the coefficients, we need to know which columns are associated with each one. If you look at `X_train` (or `X_test`), here is what you have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-1eLFroQN_8"
      },
      "outputs": [],
      "source": [
        "feature_encoder.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nwugv1WGP8M"
      },
      "outputs": [],
      "source": [
        "X_train[:5] # Visualize the first line\n",
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPI3W4ChGP8M"
      },
      "source": [
        "But how can we show it in a DataFrame? Well first, we need to use the [`.categories_`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=one%20hot%20encoder#sklearn.preprocessing.OneHotEncoder) attribute from [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=one%20hot%20encoder#sklearn.preprocessing.OneHotEncoder).\n",
        "\n",
        "Since we use `ColumnTransformer`, we need to access `OneHotEncoder` using `.transformers_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfda5Lf1GP8M"
      },
      "outputs": [],
      "source": [
        "# Access transformers from feature_encoder\n",
        "print(\"All transformers are: \", feature_encoder.transformers_)\n",
        "\n",
        "# Access one specific transformer\n",
        "print(\"One Hot Encoder transformer is: \", feature_encoder.transformers_[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbBA3sF4GP8M"
      },
      "source": [
        "Now we can simply check the categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdWtEWZCGP8M"
      },
      "outputs": [],
      "source": [
        "# Print categories (modifying code)\n",
        "categorical_column_names = np.concatenate(feature_encoder.transformers_[0][1].categories_)\n",
        "print(\"Categorical columns are: \", categorical_column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfbgjWQTGP8N"
      },
      "source": [
        "Now we can concatenate them with the numerical column names. We will use `numeric_features` variable to determine the name of our columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVEcwyPXkUWW"
      },
      "outputs": [],
      "source": [
        "numerical_column_names = X.loc[:, numerical_column].columns # using the .columns attribute gives us the name of the column\n",
        "print(\"numerical columns are: \", numerical_column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF0BuZysGP8N"
      },
      "source": [
        "Finally, we need to concatenate our `numerical_column_names` and our `categorical_column_names`. The easiest way to do it is by using [np.append](https://numpy.org/doc/stable/reference/generated/numpy.append.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g57ZltZQGP8N"
      },
      "outputs": [],
      "source": [
        "# Append all columns\n",
        "all_column_names = np.append(categorical_column_names, numerical_column_names)\n",
        "all_column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DfsrFxYGP8N"
      },
      "source": [
        "Now we can finally rank all columns by importance using coefficients ðŸ¥°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PijnHtLwGP8N"
      },
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"feature_names\": feature_encoder.get_feature_names_out(),\n",
        "    \"coefficients\":regressor.coef_\n",
        "})\n",
        "\n",
        "feature_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo4X5Fn1_Dom"
      },
      "outputs": [],
      "source": [
        "# Set coefficient to absolute values to rank features\n",
        "feature_importance[\"coefficients\"] = feature_importance[\"coefficients\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp2Sn_-6mvgc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSjmI-BJGP8N"
      },
      "outputs": [],
      "source": [
        "# Visualize ranked features using seaborn\n",
        "sns.catplot(x=\"feature_names\",\n",
        "            y=\"coefficients\",\n",
        "            data=feature_importance.sort_values(by=\"coefficients\", ascending=False),\n",
        "            kind=\"bar\",\n",
        "            aspect=100/10) # Resize graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnulXWizOJCU"
      },
      "outputs": [],
      "source": [
        "y_pred = regressor.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "# conversion rate â‚¬ => à§³ : 132\n",
        "euro_to_taka = 132\n",
        "print('Mean Absolute Error:', round(mae,2),'à§³')\n",
        "print('Mean Absolute Error:', round(mae/euro_to_taka,2),'â‚¬')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "216d08ced86f1f6e0b5764233bcb18334be12ba95b6ee555f60be9cf0be8c147"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}